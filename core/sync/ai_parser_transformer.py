"""
SolarMail - AI Parser Transformer
ML-–∞–Ω–∞–ª–∏–∑ –ø–∏—Å–µ–º –Ω–∞ –±–∞–∑–µ Hugging Face transformers
Sprint 0.3: Real neural network models for email analysis
"""

import json
import time
import warnings
from typing import Dict, List, Any, Optional, Tuple

# Suppress warnings from transformers
warnings.filterwarnings('ignore')

# Try to import transformers
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è  transformers not installed, using mock fallback")

# Fallback to mock parser if transformers unavailable
if not TRANSFORMERS_AVAILABLE:
    try:
        from ai_parser import AIParser as MockParser
        MOCK_AVAILABLE = True
    except ImportError:
        MOCK_AVAILABLE = False


class AIParserTransformer:
    """
    AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞ –±–∞–∑–µ transformer –º–æ–¥–µ–ª–µ–π
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç pre-trained –º–æ–¥–µ–ª–∏ –æ—Ç Hugging Face –¥–ª—è:
    - Sentiment analysis (—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å)
    - Text classification (–∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è)
    - Zero-shot classification (–≥–∏–±–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è)
    """
    
    def __init__(
        self,
        model_name: str = "distilbert-base-uncased-finetuned-sst-2-english",
        use_gpu: bool = False,
        fallback_to_mock: bool = True
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è transformer –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ—Ç Hugging Face
            use_gpu: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            fallback_to_mock: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mock –ø—Ä–∏ –æ—à–∏–±–∫–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        """
        self.model_name = model_name
        self.use_gpu = use_gpu and torch.cuda.is_available() if TRANSFORMERS_AVAILABLE else False
        self.fallback_to_mock = fallback_to_mock
        
        # –°—Ç–∞—Ç—É—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self.transformer_ready = False
        self.mock_parser = None
        
        # Pipelines –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
        self.sentiment_pipeline = None
        self.zero_shot_pipeline = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
        self._init_models()
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è zero-shot classification
        self.categories = [
            "work and business",
            "documents and invoices", 
            "tasks and assignments",
            "personal and social",
            "news and updates",
            "spam and promotions"
        ]
        
        # –ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ –Ω–∞—à–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ
        self.category_mapping = {
            "work and business": "Work",
            "documents and invoices": "Docs",
            "tasks and assignments": "Tasks",
            "personal and social": "People",
            "news and updates": "News",
            "spam and promotions": "Spam"
        }
    
    def _init_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è transformer –º–æ–¥–µ–ª–µ–π"""
        if not TRANSFORMERS_AVAILABLE:
            print("‚ö†Ô∏è  transformers library not available")
            self._init_fallback()
            return
        
        try:
            print(f"üß† –ó–∞–≥—Ä—É–∑–∫–∞ transformer –º–æ–¥–µ–ª–∏: {self.model_name}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º device
            device = 0 if self.use_gpu else -1
            
            # Sentiment analysis pipeline
            self.sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model=self.model_name,
                device=device
            )
            
            # Zero-shot classification –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            try:
                self.zero_shot_pipeline = pipeline(
                    "zero-shot-classification",
                    model="facebook/bart-large-mnli",
                    device=device
                )
                print("‚úÖ Zero-shot classification –∑–∞–≥—Ä—É–∂–µ–Ω")
            except Exception as e:
                print(f"‚ö†Ô∏è  Zero-shot –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self.zero_shot_pipeline = None
            
            self.transformer_ready = True
            print(f"‚úÖ Transformer –º–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã (GPU: {self.use_gpu})")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ transformer: {e}")
            self._init_fallback()
    
    def _init_fallback(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è fallback –Ω–∞ mock parser"""
        if self.fallback_to_mock and MOCK_AVAILABLE:
            print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ mock parser")
            self.mock_parser = MockParser(model_name="mock-fallback")
        else:
            print("‚ùå Fallback –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    def analyze_email(self, subject: str, body: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∏—Å—å–º–æ —Å –ø–æ–º–æ—â—å—é transformer –º–æ–¥–µ–ª–µ–π
        
        Args:
            subject: –¢–µ–º–∞ –ø–∏—Å—å–º–∞
            body: –¢–µ–ª–æ –ø–∏—Å—å–º–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å AI-–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å Sprint 0.2)
        """
        start_time = time.time()
        
        # –ï—Å–ª–∏ transformer –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
        if not self.transformer_ready:
            if self.mock_parser:
                result = self.mock_parser.analyze_email(subject, body)
                result['ai_model'] = f"{self.model_name} (mock-fallback)"
                return result
            else:
                return self._generate_empty_result()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–º—É –∏ —Ç–µ–ª–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        full_text = f"{subject or ''} {body or ''}"
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ (BERT max 512 tokens)
        full_text = full_text[:2000]  # –ø—Ä–∏–º–µ—Ä–Ω–æ 500 tokens
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        sentiment, sentiment_score = self._analyze_sentiment_transformer(full_text)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        category, category_confidence = self._analyze_category_transformer(full_text)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞ + sentiment)
        priority, priority_score = self._analyze_priority_hybrid(full_text, sentiment_score)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
        entities = self._extract_entities(subject or '', body or '')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = self._extract_keywords(full_text)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processing_time_ms = int((time.time() - start_time) * 1000)
        
        return {
            'sentiment': sentiment,
            'sentiment_score': sentiment_score,
            'priority': priority,
            'priority_score': priority_score,
            'category': category,
            'category_confidence': category_confidence,
            'entities_json': json.dumps(entities, ensure_ascii=False),
            'keywords_json': json.dumps(keywords, ensure_ascii=False),
            'ai_model': self.model_name,
            'processing_time_ms': processing_time_ms
        }
    
    def _analyze_sentiment_transformer(self, text: str) -> Tuple[str, float]:
        """
        –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é transformer
        
        Returns:
            Tuple (sentiment, score)
        """
        if not text.strip():
            return 'neutral', 0.5
        
        try:
            result = self.sentiment_pipeline(text)[0]
            label = result['label'].lower()
            score = result['score']
            
            # –ú–∞–ø–ø–∏–Ω–≥ POSITIVE/NEGATIVE –Ω–∞ –Ω–∞—à–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if label == 'positive':
                return 'positive', score
            elif label == 'negative':
                return 'negative', 1.0 - score  # –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º score –¥–ª—è negative
            else:
                return 'neutral', 0.5
                
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ sentiment analysis: {e}")
            return 'neutral', 0.5
    
    def _analyze_category_transformer(self, text: str) -> Tuple[str, float]:
        """
        –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –ø–æ–º–æ—â—å—é zero-shot classification
        
        Returns:
            Tuple (category, confidence)
        """
        if not self.zero_shot_pipeline or not text.strip():
            return 'General', 0.5
        
        try:
            result = self.zero_shot_pipeline(
                text,
                candidate_labels=self.categories,
                multi_label=False
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –ª—É—á—à—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            best_category_raw = result['labels'][0]
            confidence = result['scores'][0]
            
            # –ú–∞–ø–ø–∏–º –Ω–∞ –Ω–∞—à–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category = self.category_mapping.get(best_category_raw, 'General')
            
            return category, confidence
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ category analysis: {e}")
            return 'General', 0.5
    
    def _analyze_priority_hybrid(self, text: str, sentiment_score: float) -> Tuple[str, float]:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ + sentiment)
        
        Returns:
            Tuple (priority, score)
        """
        text_lower = text.lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤—ã—Å–æ–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        high_priority_words = [
            'urgent', '—Å—Ä–æ—á–Ω–æ', '–≤–∞–∂–Ω–æ', 'critical', 'asap',
            'deadline', '–¥–µ–¥–ª–∞–π–Ω', 'emergency', 'immediately'
        ]
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        medium_priority_words = [
            '–≤–∞–∂–Ω—ã–π', '–Ω—É–∂–Ω–æ', '—Ç—Ä–µ–±—É–µ—Ç—Å—è', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ',
            'action required', 'please review'
        ]
        
        # –°—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        high_count = sum(1 for word in high_priority_words if word in text_lower)
        medium_count = sum(1 for word in medium_priority_words if word in text_lower)
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (–ø—Ä–æ–±–ª–µ–º—ã = –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        negative_boost = 0.2 if sentiment_score < 0.4 else 0.0
        
        if high_count > 0:
            score = min(0.7 + (high_count * 0.1) + negative_boost, 1.0)
            return 'high', score
        elif medium_count > 0:
            score = min(0.4 + (medium_count * 0.1) + negative_boost, 0.7)
            return 'medium', score
        else:
            return 'low', 0.3
    
    def _extract_entities(self, subject: str, body: str) -> Dict[str, List[str]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (–±–∞–∑–æ–≤—ã–µ regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
        
        –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å NER –º–æ–¥–µ–ª–∏
        """
        import re
        
        text = f"{subject} {body}"
        
        entities = {
            'emails': [],
            'dates': [],
            'urls': [],
            'persons': []
        }
        
        # Email –∞–¥—Ä–µ—Å–∞
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        entities['emails'] = list(set(re.findall(email_pattern, text)))[:10]
        
        # –î–∞—Ç—ã
        date_patterns = [
            r'\d{4}-\d{2}-\d{2}',
            r'\d{2}\.\d{2}\.\d{4}',
            r'\d{1,2}/\d{1,2}/\d{4}'
        ]
        for pattern in date_patterns:
            entities['dates'].extend(re.findall(pattern, text))
        entities['dates'] = list(set(entities['dates']))[:10]
        
        # URLs
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
        entities['urls'] = list(set(re.findall(url_pattern, text)))[:10]
        
        return entities
    
    def _extract_keywords(self, text: str) -> Dict[str, List[str]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        
        –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TF-IDF –∏–ª–∏ KeyBERT
        """
        import re
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'is', 'was', 'are', 'were', 'been',
            '–≤', '–∏', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–∫', '–∏–∑', '—ç—Ç–æ', '–±—ã—Ç—å'
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞
        words = re.findall(r'\b\w+\b', text.lower())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º
        word_freq = {}
        for word in words:
            if len(word) > 3 and word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # –¢–æ–ø-10 —Å–ª–æ–≤
        top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        keywords_list = [word for word, freq in top_keywords]
        
        return {
            'keywords': keywords_list,
            'topics': []  # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å topic modeling
        }
    
    def _generate_empty_result(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π"""
        return {
            'sentiment': 'neutral',
            'sentiment_score': 0.5,
            'priority': 'low',
            'priority_score': 0.3,
            'category': 'General',
            'category_confidence': 0.5,
            'entities_json': json.dumps({'emails': [], 'dates': [], 'urls': [], 'persons': []}, ensure_ascii=False),
            'keywords_json': json.dumps({'keywords': [], 'topics': []}, ensure_ascii=False),
            'ai_model': f"{self.model_name} (unavailable)",
            'processing_time_ms': 0
        }
    
    def batch_analyze(self, emails: List[Dict]) -> List[Dict]:
        """
        –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∏—Å–µ–º
        
        Args:
            emails: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–æ–ª—è–º–∏ 'subject' –∏ 'body_preview'
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å AI-–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        results = []
        
        for email in emails:
            subject = email.get('subject', '')
            body = email.get('body_preview', '')
            
            meta = self.analyze_email(subject, body)
            results.append(meta)
        
        return results
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö
        """
        return {
            'transformer_ready': self.transformer_ready,
            'transformers_available': TRANSFORMERS_AVAILABLE,
            'model_name': self.model_name,
            'gpu_enabled': self.use_gpu,
            'sentiment_pipeline': self.sentiment_pipeline is not None,
            'zero_shot_pipeline': self.zero_shot_pipeline is not None,
            'mock_fallback': self.mock_parser is not None,
            'version': '0.3.0',
            'type': 'transformer-ml' if self.transformer_ready else 'mock-fallback'
        }


def demo_transformer():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã transformer –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    print("=" * 70)
    print("üß† SolarMail AI Parser Transformer - Demo")
    print("=" * 70)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    parser = AIParserTransformer(fallback_to_mock=True)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
    model_info = parser.get_model_info()
    print(f"\nüìä Model Info:")
    print(f"   Transformer Ready: {'‚úÖ' if model_info['transformer_ready'] else '‚ùå'}")
    print(f"   Model: {model_info['model_name']}")
    print(f"   GPU: {'‚úÖ' if model_info['gpu_enabled'] else '‚ùå'}")
    print(f"   Type: {model_info['type']}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø–∏—Å—å–º–∞
    test_emails = [
        {
            'subject': 'Urgent: Critical bug in production',
            'body': 'We have a critical issue in production that needs immediate attention. The payment system is down and customers cannot complete purchases.'
        },
        {
            'subject': 'Thank you for the amazing work!',
            'body': 'I wanted to express my gratitude for the excellent job you did on the project. Everything exceeded our expectations!'
        },
        {
            'subject': 'Invoice #12345 - Payment Due',
            'body': 'Please find attached the invoice for services rendered. Payment is due by end of month.'
        },
        {
            'subject': '–°—Ä–æ—á–Ω–æ: —Ç—Ä–µ–±—É–µ—Ç—Å—è –≤–∞—à–µ —Ä–µ—à–µ–Ω–∏–µ',
            'body': '–î–æ–±—Ä—ã–π –¥–µ–Ω—å! –ù–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –ø–æ –¥–µ–¥–ª–∞–π–Ω—É –ø—Ä–æ–µ–∫—Ç–∞ SolarMail. –ü—Ä–æ—à—É —Å—Ä–æ—á–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å.'
        }
    ]
    
    print("\nüìß –ê–Ω–∞–ª–∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–∏—Å–µ–º:\n")
    
    for i, email in enumerate(test_emails, 1):
        print(f"{i}. –¢–µ–º–∞: {email['subject']}")
        
        meta = parser.analyze_email(email['subject'], email['body'])
        
        print(f"   üéØ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {meta['priority']} (score: {meta['priority_score']:.2f})")
        print(f"   üìÅ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {meta['category']} (confidence: {meta['category_confidence']:.2f})")
        print(f"   üòä –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {meta['sentiment']} (score: {meta['sentiment_score']:.2f})")
        print(f"   ü§ñ –ú–æ–¥–µ–ª—å: {meta['ai_model']}")
        print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {meta['processing_time_ms']} ms")
        print()
    
    print("=" * 70)


if __name__ == "__main__":
    demo_transformer()

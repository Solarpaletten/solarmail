"""
SolarMail - AI Parser Module
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∏—Å–µ–º: —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, –∫–∞—Ç–µ–≥–æ—Ä–∏—è, —Å—É—â–Ω–æ—Å—Ç–∏
Sprint 0.2: Mock-–∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
"""

import re
import json
import time
from typing import Dict, List, Any, Optional
from datetime import datetime


class AIParser:
    """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–∏—Å–µ–º"""
    
    def __init__(self, model_name: str = "dashka-solar-mini"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI Parser
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        """
        self.model_name = model_name
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        self.priority_keywords = {
            'high': [
                'urgent', '—Å—Ä–æ—á–Ω–æ', '–≤–∞–∂–Ω–æ', 'critical', 'asap', 
                'deadline', '–¥–µ–¥–ª–∞–π–Ω', 'emergency', 'immediately',
                '—Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ', '–ø—Ä–æ—à—É —Å—Ä–æ—á–Ω–æ'
            ],
            'medium': [
                '–≤–∞–∂–Ω—ã–π', '–Ω—É–∂–Ω–æ', '—Ç—Ä–µ–±—É–µ—Ç—Å—è', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ',
                'please review', 'action required', '–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ'
            ]
        }
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        self.category_keywords = {
            'Work': [
                'meeting', '–≤—Å—Ç—Ä–µ—á–∞', 'project', '–ø—Ä–æ–µ–∫—Ç', 'task', '–∑–∞–¥–∞—á–∞',
                'deadline', '–¥–µ–¥–ª–∞–π–Ω', 'report', '–æ—Ç—á–µ—Ç', 'presentation',
                '–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è', 'conference', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', 'sprint',
                'review', '—Ä–µ–≤—å—é', 'merge', 'deploy', 'code', '–∫–æ–¥'
            ],
            'Docs': [
                'invoice', '—Å—á–µ—Ç', 'contract', '–¥–æ–≥–æ–≤–æ—Ä', 'agreement',
                'document', '–¥–æ–∫—É–º–µ–Ω—Ç', 'pdf', 'file', '—Ñ–∞–π–ª',
                'attachment', '–≤–ª–æ–∂–µ–Ω–∏–µ', 'scan', '—Å–∫–∞–Ω'
            ],
            'Tasks': [
                'todo', '–¥–µ–ª–∞—Ç—å', 'task', '–∑–∞–¥–∞–Ω–∏–µ', 'action item',
                'assign', '–Ω–∞–∑–Ω–∞—á–µ–Ω–æ', 'complete', '–∑–∞–≤–µ—Ä—à–∏—Ç—å',
                'issue', '—Ç–∏–∫–µ—Ç', 'bug', '–±–∞–≥', 'fix', '–∏—Å–ø—Ä–∞–≤–∏—Ç—å'
            ],
            'People': [
                'birthday', '–¥–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è', 'congratulations', '–ø–æ–∑–¥—Ä–∞–≤–ª—è–µ–º',
                'welcome', '–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å', 'hello', '–ø—Ä–∏–≤–µ—Ç',
                'thanks', '—Å–ø–∞—Å–∏–±–æ', 'thank you', 'regards'
            ],
            'News': [
                'newsletter', '–Ω–æ–≤–æ—Å—Ç–∏', 'update', '–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ',
                'announcement', '–æ–±—ä—è–≤–ª–µ–Ω–∏–µ', 'release', '—Ä–µ–ª–∏–∑',
                'version', '–≤–µ—Ä—Å–∏—è', 'changelog'
            ],
            'Spam': [
                'unsubscribe', '–æ—Ç–ø–∏—Å–∞—Ç—å—Å—è', 'discount', '—Å–∫–∏–¥–∫–∞',
                'offer', '–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ', 'win', '–≤—ã–∏–≥—Ä–∞—Ç—å', 'prize',
                'click here', '–Ω–∞–∂–º–∏—Ç–µ –∑–¥–µ—Å—å', 'free', '–±–µ—Å–ø–ª–∞—Ç–Ω–æ'
            ]
        }
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        self.sentiment_keywords = {
            'positive': [
                'thanks', '—Å–ø–∞—Å–∏–±–æ', 'great', '–æ—Ç–ª–∏—á–Ω–æ', 'excellent',
                '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', 'good', '—Ö–æ—Ä–æ—à–æ', 'perfect', '–∏–¥–µ–∞–ª—å–Ω–æ',
                'love', '–Ω—Ä–∞–≤–∏—Ç—Å—è', 'amazing', '–ø–æ—Ç—Ä—è—Å–∞—é—â–µ', 'wonderful',
                'appreciate', '—Ü–µ–Ω—é', 'congratulations', '–ø–æ–∑–¥—Ä–∞–≤–ª—è—é'
            ],
            'negative': [
                'problem', '–ø—Ä–æ–±–ª–µ–º–∞', 'issue', '–æ—à–∏–±–∫–∞', 'error',
                'failed', '–ø—Ä–æ–≤–∞–ª–µ–Ω–æ', 'wrong', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ', 'bad',
                '–ø–ª–æ—Ö–æ', 'terrible', '—É–∂–∞—Å–Ω–æ', 'disappointed', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω',
                'complaint', '–∂–∞–ª–æ–±–∞', 'urgent', '—Å—Ä–æ—á–Ω–æ', 'critical'
            ]
        }
        
    def analyze_email(self, subject: str, body: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∏—Å—å–º–æ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        
        Args:
            subject: –¢–µ–º–∞ –ø–∏—Å—å–º–∞
            body: –¢–µ–ª–æ –ø–∏—Å—å–º–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å preview)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å AI-–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        start_time = time.time()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–º—É –∏ —Ç–µ–ª–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        full_text = f"{subject or ''} {body or ''}".lower()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        priority, priority_score = self._analyze_priority(full_text)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        category, category_confidence = self._analyze_category(full_text)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        sentiment, sentiment_score = self._analyze_sentiment(full_text)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
        entities = self._extract_entities(subject or '', body or '')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = self._extract_keywords(full_text)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processing_time_ms = int((time.time() - start_time) * 1000)
        
        return {
            'sentiment': sentiment,
            'sentiment_score': sentiment_score,
            'priority': priority,
            'priority_score': priority_score,
            'category': category,
            'category_confidence': category_confidence,
            'entities_json': json.dumps(entities, ensure_ascii=False),
            'keywords_json': json.dumps(keywords, ensure_ascii=False),
            'ai_model': self.model_name,
            'processing_time_ms': processing_time_ms
        }
    
    def _analyze_priority(self, text: str) -> tuple[str, float]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–∏—Å—å–º–∞
        
        Returns:
            Tuple (priority, score)
        """
        high_count = sum(1 for kw in self.priority_keywords['high'] if kw in text)
        medium_count = sum(1 for kw in self.priority_keywords['medium'] if kw in text)
        
        if high_count > 0:
            score = min(0.7 + (high_count * 0.1), 1.0)
            return 'high', score
        elif medium_count > 0:
            score = min(0.4 + (medium_count * 0.1), 0.7)
            return 'medium', score
        else:
            return 'low', 0.3
    
    def _analyze_category(self, text: str) -> tuple[str, float]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–∏—Å—å–º–∞
        
        Returns:
            Tuple (category, confidence)
        """
        category_scores = {}
        
        for category, keywords in self.category_keywords.items():
            score = sum(1 for kw in keywords if kw in text)
            if score > 0:
                category_scores[category] = score
        
        if not category_scores:
            return 'General', 0.5
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å–∫–æ—Ä–æ–º
        best_category = max(category_scores, key=category_scores.get)
        max_score = category_scores[best_category]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º confidence (0.5 - 1.0)
        confidence = min(0.5 + (max_score * 0.15), 1.0)
        
        return best_category, confidence
    
    def _analyze_sentiment(self, text: str) -> tuple[str, float]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–∏—Å—å–º–∞
        
        Returns:
            Tuple (sentiment, score)
        """
        positive_count = sum(1 for kw in self.sentiment_keywords['positive'] if kw in text)
        negative_count = sum(1 for kw in self.sentiment_keywords['negative'] if kw in text)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –±–∞–ª–∞–Ω—Å
        total = positive_count + negative_count
        
        if total == 0:
            return 'neutral', 0.5
        
        if positive_count > negative_count:
            score = min(0.5 + (positive_count * 0.1), 1.0)
            return 'positive', score
        elif negative_count > positive_count:
            score = max(0.5 - (negative_count * 0.1), 0.0)
            return 'negative', score
        else:
            return 'neutral', 0.5
    
    def _extract_entities(self, subject: str, body: str) -> Dict[str, List[str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –ø–∏—Å—å–º–∞ (—ç–º–µ–π–ª—ã, –¥–∞—Ç—ã, –∏–º–µ–Ω–∞)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ç–∏–ø–∞–º–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π
        """
        text = f"{subject} {body}"
        
        entities = {
            'emails': [],
            'dates': [],
            'urls': [],
            'persons': []
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º email –∞–¥—Ä–µ—Å–∞
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        entities['emails'] = list(set(re.findall(email_pattern, text)))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—ã (–ø—Ä–æ—Å—Ç—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã)
        date_patterns = [
            r'\d{4}-\d{2}-\d{2}',  # 2025-10-25
            r'\d{2}\.\d{2}\.\d{4}',  # 25.10.2025
            r'\d{1,2}/\d{1,2}/\d{4}'  # 10/25/2025
        ]
        for pattern in date_patterns:
            entities['dates'].extend(re.findall(pattern, text))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º URLs
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
        entities['urls'] = list(set(re.findall(url_pattern, text)))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–µ–Ω–∞ (–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞, –ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        # –ò—â–µ–º —Å–ª–æ–≤–∞ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏–¥—É—Ç –ø–æ–¥—Ä—è–¥ (–∏–º—è + —Ñ–∞–º–∏–ª–∏—è)
        name_pattern = r'\b[A-Z–ê-–Ø–Å][a-z–∞-—è—ë]+(?:\s+[A-Z–ê-–Ø–Å][a-z–∞-—è—ë]+)\b'
        potential_names = re.findall(name_pattern, text)
        # –§–∏–ª—å—Ç—Ä—É–µ–º —á–∞—Å—Ç—ã–µ –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
        stop_words = {'Subject', 'From', 'To', 'Date', 'Best Regards', 'Thank You'}
        entities['persons'] = [name for name in potential_names if name not in stop_words][:5]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—É—â–Ω–æ—Å—Ç–µ–π
        for key in entities:
            entities[key] = entities[key][:10]
        
        return entities
    
    def _extract_keywords(self, text: str) -> Dict[str, List[str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        """
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'is', 'was', 'are', 'were', 'been',
            '–≤', '–∏', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–∫', '–∏–∑', '—ç—Ç–æ', '–±—ã—Ç—å'
        }
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
        words = re.findall(r'\b\w+\b', text.lower())
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤ (–∏—Å–∫–ª—é—á–∞—è —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ)
        word_freq = {}
        for word in words:
            if len(word) > 3 and word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø-10 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
        top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        keywords_list = [word for word, freq in top_keywords]
        
        return {
            'keywords': keywords_list,
            'topics': self._infer_topics(keywords_list)
        }
    
    def _infer_topics(self, keywords: List[str]) -> List[str]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–º
        """
        topic_mapping = {
            'development': ['code', 'develop', 'git', 'branch', 'deploy', 'test'],
            '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞': ['–∫–æ–¥', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', 'git', '–≤–µ—Ç–∫–∞', '–¥–µ–ø–ª–æ–π', '—Ç–µ—Å—Ç'],
            'management': ['project', 'meeting', 'deadline', 'plan', 'sprint'],
            '–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç': ['–ø—Ä–æ–µ–∫—Ç', '–≤—Å—Ç—Ä–µ—á–∞', '–¥–µ–¥–ª–∞–π–Ω', '–ø–ª–∞–Ω', '—Å–ø—Ä–∏–Ω—Ç'],
            'finance': ['invoice', 'payment', 'budget', 'cost', 'price'],
            '—Ñ–∏–Ω–∞–Ω—Å—ã': ['—Å—á–µ—Ç', '–æ–ø–ª–∞—Ç–∞', '–±—é–¥–∂–µ—Ç', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞']
        }
        
        topics = []
        for topic, topic_keywords in topic_mapping.items():
            if any(kw in keywords for kw in topic_keywords):
                topics.append(topic)
        
        return topics[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 —Ç–µ–º—ã
    
    def batch_analyze(self, emails: List[Dict]) -> List[Dict]:
        """
        –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∏—Å–µ–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Args:
            emails: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–æ–ª—è–º–∏ 'subject' –∏ 'body_preview'
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å AI-–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        results = []
        
        for email in emails:
            subject = email.get('subject', '')
            body = email.get('body_preview', '')
            
            meta = self.analyze_email(subject, body)
            results.append(meta)
        
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        return {
            'model_name': self.model_name,
            'version': '0.2.0',
            'type': 'mock-heuristic',
            'categories': list(self.category_keywords.keys()),
            'priority_levels': ['high', 'medium', 'low'],
            'sentiment_types': ['positive', 'neutral', 'negative']
        }


def demo_analysis():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã AI Parser"""
    print("=" * 60)
    print("üß† SolarMail AI Parser - Demo")
    print("=" * 60)
    
    parser = AIParser()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø–∏—Å—å–º–∞
    test_emails = [
        {
            'subject': '–°—Ä–æ—á–Ω–æ: —Ç—Ä–µ–±—É–µ—Ç—Å—è –≤–∞—à–µ —Ä–µ—à–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–µ–∫—Ç—É',
            'body': '–î–æ–±—Ä—ã–π –¥–µ–Ω—å! –ù–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –ø–æ –¥–µ–¥–ª–∞–π–Ω—É –ø—Ä–æ–µ–∫—Ç–∞ SolarMail. –ü—Ä–æ—à—É —Å—Ä–æ—á–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å.'
        },
        {
            'subject': '–°–ø–∞—Å–∏–±–æ –∑–∞ –æ—Ç–ª–∏—á–Ω—É—é —Ä–∞–±–æ—Ç—É!',
            'body': '–•–æ—á—É –ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –∫–æ–º–∞–Ω–¥—É –∑–∞ –ø—Ä–µ–∫—Ä–∞—Å–Ω—É—é —Ä–∞–±–æ—Ç—É –Ω–∞–¥ –ø—Ä–æ–µ–∫—Ç–æ–º. –í—Å—ë —Å–¥–µ–ª–∞–Ω–æ –Ω–∞ –≤—ã—Å—à–µ–º —É—Ä–æ–≤–Ω–µ!'
        },
        {
            'subject': 'Invoice #12345 for October',
            'body': 'Please find attached the invoice for services rendered in October. Payment due by end of month.'
        },
        {
            'subject': 'Newsletter: Weekly Tech Updates',
            'body': 'Check out this week\'s top tech news and updates from the industry. New AI models released!'
        }
    ]
    
    print("\nüìß –ê–Ω–∞–ª–∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–∏—Å–µ–º:\n")
    
    for i, email in enumerate(test_emails, 1):
        print(f"{i}. –¢–µ–º–∞: {email['subject']}")
        
        meta = parser.analyze_email(email['subject'], email['body'])
        
        print(f"   üéØ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {meta['priority']} (score: {meta['priority_score']:.2f})")
        print(f"   üìÅ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {meta['category']} (confidence: {meta['category_confidence']:.2f})")
        print(f"   üòä –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {meta['sentiment']} (score: {meta['sentiment_score']:.2f})")
        print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {meta['processing_time_ms']} ms")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º entities
        entities = json.loads(meta['entities_json'])
        if any(entities.values()):
            print(f"   üîç –°—É—â–Ω–æ—Å—Ç–∏: {', '.join([f'{k}={len(v)}' for k, v in entities.items() if v])}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º keywords
        keywords = json.loads(meta['keywords_json'])
        if keywords['keywords']:
            print(f"   üè∑Ô∏è  –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(keywords['keywords'][:5])}")
        
        print()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞
    print("=" * 60)
    stats = parser.get_stats()
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞:")
    print(f"   –ú–æ–¥–µ–ª—å: {stats['model_name']}")
    print(f"   –í–µ—Ä—Å–∏—è: {stats['version']}")
    print(f"   –¢–∏–ø: {stats['type']}")
    print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(stats['categories'])}")
    print("=" * 60)


if __name__ == "__main__":
    demo_analysis()
